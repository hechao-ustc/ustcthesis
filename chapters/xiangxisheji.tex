% !TeX root = ../main.tex

\chapter{系统详细设计与实现}

在前两章中完成需求分析以及系统概要设计之后，本章将详细介绍各个功能
模块重难点的设计与实现。针对每一个功能模块，首先进行功能流程描述，然后对系
统实现类进行设计并阐述，通过类图介绍系统对象之间的交互顺序，最后进行系统实现的效果图展示。

\section{数据源管理}

数据源是入湖任务的前提要求，是Iceberg入湖任务的源头，注册相应数据源，创建入湖任务时即可根据创建的数据源选择对应的源表。
从数据源分类上来看，数据湖分析系统支持关系型数据库源（mysql）以及消息队列数据源（tube、kafka、pulsar），
该模块支持数据源创建、查看、编辑、删除功能。数据源创建的相关页面如图5.1所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{创建数据源.png}
  \caption{创建数据源页面}
  \label{fig:badge}
\end{figure}

在数据源管理系统模块中，用户首先需要登录系统，通过安全中心认证服务进
行身份认证。成功登录系统之后，进行数据源信息录入，基本信息包括
数据源名称和描述，不同的数据源所需要的详细信息也不一样，以关系型数据库MySQL为例，需
要配置mysql用户名、密码、库名、服务器地址和服务器端口。完成配置信息设置之后，
会将数据源持久化保存在数据库中。流程图如图5.2所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.3\textwidth]{创建数据源流程图.png}
  \caption{创建数据源流程图}
  \label{fig:badge}
\end{figure}

数据源管理是基于SpringBoot框架的Web应用，系统的主要类图如图5.3所示：

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{数据源类图.png}
  \caption{数据源管理类图}
  \label{fig:badge}
\end{figure}

DatasourceV2Controller是数据源管理模块的核心类，是用户访问数据源管理的入口，
主要负责接受各种数据源的创建、查看、更新、删除以及连接测试的请求，对应的逻辑处理是在
成员变量DsV2BizService中实现的。

\section{元数据管理}

元数据是描述其他数据的数据，它提供有关信息资源的结构数据。它用于识别、
评价和追踪资源，实现简单高效地管理大量网络化数据，并有效地发现、查找、组织和管理信息资源。

这里的元数据是描述Iceberg表的，Iceberg表即目标表，
是入湖任务的前提要求，可以创建新表或关联已有的表。元数据管理模块主要管理Iceberg元数据，
包括数据优化、表的创建、编辑和查看四个主要功能。其中数据优化功能将单独介绍。表的创建可在数据湖分析
系统上创建新的Iceberg表，也可以关联在其他平台上已创建的Iceberg表。表的编辑支持对已创建的表进行
修改和编辑，包括增加和删除字段操作。元数据管理的相关页面如图5.4所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{元数据管理.png}
  \caption{元数据管理页面}
  \label{fig:badge}
\end{figure}

在元数据管理系统模块中，用户在对元数据进行操作前，都必须通过安全中心认证服务，
因为Iceberg元数据是存储在hive metastore中的，而对hive metastore中的元数据修改在企业内
是比较敏感的，所以必须添加身份认证以及权限认证来确保安全问题。
以Iceberg表的创建和修改来说，表的创建需要和hive metastore、mysql进行交互，一个createTable的request建立后，首先会判断
metastore中是否已存在表，若不存在，则会判断MySQL中是否存在对应的DB和table，这里MySQL中的DB和
table实质上是一个映射，是用来方便在数据湖分析系统中展示表元数据的，若MySQL中不存在，则创建对应的DB和table，
接着会在hive metastore中创建实际的Iceberg表元数据，表的修改如果涉及到字段的增加及删除操作，则会更新hive metastore中对应的表元数据。
这其中涉及到的对hive metastore进行操作前都会进行安全中心的身份认证和权限认证。流程图如图5.5所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{表元数据创建流程图.png}
  \caption{Iceberg表元数据创建流程图}
  \label{fig:badge}
\end{figure}

元数据管理系统中主要类图如图5.6所示：

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{MetadataControllerV2.png}
  \caption{元数据管理类图}
  \label{fig:badge}
\end{figure}

MetadataControllerV2是元数据管理的核心类，是用户访问元数据管理的入口，
主要负责Iceberg表的创建、修改、查看、数据优化等功能。在核心类的成员变量中，
TableService负责Iceberg表的创建、修改、查看的逻辑处理；DataBaseService
负责数据库相关操作的逻辑处理；SecurityCenterService负责安全中心认证的逻辑处理，
安全中心认证是所有操作元数据的前提，必须通过认证后，才能对表的结构以及数据进行查看或者修改；
OMSService是元数据服务，是基于hive metastore实现的；
OptimizerService是表的自动优化的逻辑处理，会在后面进行详细介绍。


\section{数据入湖}

数据入湖功能模块是DLA系统核心流程功能，目的是用户通过该功能将源数据表流程化入湖
以及查看已申请入湖任务执行情况。其中入湖任务列表中具有操作任务的功能，
并且根据数据入湖方式具有不同状态展示，以供用户查看和处理。入湖任务列表如图5.7所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{数据入湖.png}
  \caption{数据入湖列表}
  \label{fig:badge}
\end{figure}

对于每一个入湖任务，都需要进行五步：
第一步是进行基础信息的填写，根据入湖需要选择对应的任务类型，填写必要的任务名；
第二步是填写源表的信息，其中实时入湖与关系型数据库入湖需要提前注册数据源，选择对应数据源表；
第三步是填写目标表的信息，若目标表已经创建，则会自动填充信息，若不存在，则会自动创建Iceberg表，无需提前新建表，这大大优化了入湖任务的流程；
第四步是进行映射预览，会对源表及目标表的schema进行比对，若schema不一致将无法进行下一步；
第五步是配置参数及资源。相关的流程图如图5.8所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{任务创建流程图.png}
  \caption{入湖任务创建流程图}
  \label{fig:badge}
\end{figure}

针对不同的任务类型，我们分别对接了内部的不同任务平台，通过各个平台的REST API的方式实现不同任务的创建。
若为实时数据入湖任务，则会在实时流计算平台Oceanus上创建对应的实时任务来进行Iceberg表的写入，
Oceanus底层使用的引擎是flink；若为存量数据入湖或者关系
型数据入湖，则会在统一调度平台US上创建定时任务来进行Iceberg表的写入，底层使用的引擎是spark，
并且可在系统上设置时间间隔和起始时间。相关的核心类图如图5.9所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{TaskControllerV2.png}
  \caption{数据入湖类图}
  \label{fig:badge}
\end{figure}

可以看到，TaskControllerV2是数据入湖管理的核心类，是用户进行任务创建的入口，负责入湖任务的
创建、查看修改等功能，对应的逻辑处理是在TaskBizV2Service中实现的。在成员变量中，SecurityCenterService
是负责安全中心认证的；OceanusJobService是负责管理实时任务的，对接的是实时任务平台的api；
USJobV2Service是负责管理存量数据入湖任务的，对接的是统一调度平台US的api。

\section{数据探索}

数据探索是数据入湖后，用户需要进行数据查看或者数据分析时使用的工具，目前主要依赖内部的统一查询平台实现数据探索功能，
通过查询平台的SuperSQL，用户只需简单的配置参数(set supersql.execution.engine = presto / spark)，
就可以轻松通过Presto或者Spark查询分析数据。SuperSQL做到计算对用户透明，
避免用户在不同系统中的切换成本和高昂的学习成本。SQL语法采用标准的SQL语法。例如图5.10。
除了可以使用标准的sql进行数据探索外，统一查询平台还提供了Jupyter Notebook的方式，可以在Jupyter中使用Pyspark进行
数据的探索，该种方式与用户的交互性比较好，可以使用各种图表使数据可视化。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{presto标准sql.png}
  \caption{Presto查询sql}
  \label{fig:badge}
\end{figure}

除了内部的统一查询平台可以进行数据探索外，我们还在数据湖分析平台中内嵌了Zeppelin来提供数据探索。
Zeppelin是一个基于Web的notebook，提供交互数据分析和可视化。后台支持接
入多种数据处理引擎，如spark，hive等。支持多种语言： Scala(Apache Spark)、
Python(Apache Spark)、SparkSQL、 Hive、 Markdown、Shell等。
开发者可以通过实现更多的解释器来为Zeppelin添加数据引擎。Zeppelin的使用页面如图5.11所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{Zeppelin使用.png}
  \caption{Zeppelin使用页面}
  \label{fig:badge}
\end{figure}

\section{自动优化服务}

目前使用Iceberg的用户需要自己手写java代码来维护Iceberg表，
用户认为Iceberg的运维成本较高，学习成本较大，所以设计了数据自动优化服务。
数据优化服务的目的是降低用户的运维成本，使用户可以一
键启动该服务，不需要自己写java程序来优化，并会根据表的若干指标及历史执行情况
判断所需资源，并配有告警机制，及时通知专业运维处理。
当前服务包括小文件合并、历史快照清理、孤儿文件删除、数据生命周期管理等，
其中我们在小文件合并这方面做了很多的优化，会在接下来进行介绍。

在概要设计中，我们介绍了整个自动优化服务的总体运行流程图，根据流程图，我们知道Iceberg需要将
当前文件状态发布到外部系统，而Iceberg当前还不支持将当前文件状态发布到外部系统，
因此我们基于Iceberg的内核重新设计了不同阶段的不同事件，并针对不同的事件定义相应的指标，
通过事件汇报框架，将这些指标汇报到自动优化服务，由服务根据事件的类型来决定进行什么类型的优化和任务执行粒度，
例如发生了 NewSnapshotEvent 5次，则说明触发了5次文件的commit操作，那么这时候就可以执行一次小文件合并。
整体事件汇报的框架如图5.12所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{Iceberg事件汇报框架.png}
  \caption{Iceberg事件汇报框架}
  \label{fig:badge}
\end{figure}

以NewCreateSnapshotEvent为例，如图5.13所示，当前事件里面可以看到一个snapshot的事件信息包含新增了多少文件、
新增了多少records、新增的文件size、总的文件size等，则下游的service拿到这些event以后可以
解析开这些数据，然后配置相应的规则去触发不同的合并任务。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{snapshot事件信息.png}
  \caption{snapshot事件信息}
  \label{fig:badge}
\end{figure}

\subsection{合并小文件}

合并⼩⽂件主要有四个过程：

（1）根据条件（各个分区的MSE、阈值T）筛选出⽂件；

（2）根据targetSize划分task；

（3）合并⼩⽂件；

（4）返回数据到RewriteDataFilesActionResult中；

通过概要设计中的公式计算得到各个分区的MSE值之后，通过事件汇报的方式发送到文件合并服务，后台服务在收到这些
MSE之后自动根据不同的表所配置的不同的值来决策是否需要对某个分区或者是某张表进行合并。
commit事件为一次snapshot生成的事件，通过计算每个snapshot里面的文件状态并通过
summary的方式记录通过事件发送到后端服务，从而使得服务能够清楚的了解当前每一张表的
snapshot中的文件状态，以此来作为是否触发合并的判断依据，Iceberg Commit事件结构如图5.14所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{Iceberg Commit事件结构.png}
  \caption{Iceberg Commit事件结构}
  \label{fig:badge}
\end{figure}

小文件合并相关的类图如图5.15所示。MSE是针对每个分区进行计算的，所有会有一个存放partition的list；
RewriteConfig是用于配置参数修改的，比如目标文件的大小、计算阈值T的比例系数等；
buildActionResult会将表名、size大小、删除的文件、增加的文件封装起来交给下游的指标系统；
doExecute会调用Iceberg那边提供的重写文件接口进行实际的文件合并操作。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{BaseRewriteDataFilesJob.png}
  \caption{小文件合并相关类图}
  \label{fig:badge}
\end{figure}

经过这样的优化后。如图5.16，和之前的定时调度进行合并小文件相比，可以看到基于MSE的小文件合并在文件数量上得到了很好的平衡。
如图5.17所示，合并优化的文件状态均在一个均衡的水平，
小文件的增长得到了有效的控制，离线合并任务只在MSE达到某个阈值的时候才触发合并，避免了不必要的计算资源的开销。

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{不同时刻作业合并小文件数.png}
  \caption{不同时刻作业合并小文件数}
  \label{fig:badge}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{随着时间的执行小文件个数的变化.png}
  \caption{随着时间的执行小文件个数的变化}
  \label{fig:badge}
\end{figure}

\subsection{历史快照清理}

对于历史快照清理，我们设置了两个参数来控制快照删除，分别是dla.optimizer.expire.snapshot.seconds
和dla.optimizer.expire.snapshot.num，表示快照失效时间和快照保留数量，达到任何一个条件都将进行快照的清除。
被删除的snapshot将不能进⾏time travel（时间旅⾏），
当将元数据中的cleanExpiredFiles设置为true，物理⽂件也将被删除；
ExpireSnapshot删除过期快照步骤有两步：

（1）调⽤RemoveSnapshots的commit删除快照，当cleanExpiredFiles设置为true时删除对应的物理⽂件；

（2）将数据写⼊到result中。

历史快照清理的相关类图如图5.18所示。
buildActionResult会将表名、删除的dataFiles、删除的manifestFiles、删除的manifestLists封装起来交给下游的指标系统；
doExecute会调用Iceberg那边提供的文件清除接口进行实际的文件删除操作。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{ExpireSnapshotsJob.png}
  \caption{历史快照清理相关类图}
  \label{fig:badge}
\end{figure}

\subsection{孤儿文件删除}

清理孤⼉⽂件过程分为三步：

（1）筛选出合规的数据⽂件（数据及元数据）validFileDF，通过⽂件fs client（hdfs/cos）
筛出过期的⾮隐藏⽂件（以"下划线"和"."开头的⽂件）actualFileDF；

（2）将actualFileDF与validFileDF和他们相交的公共部分joinCond做left anti join，最终的⽂件即orphanFiles，并将此⽂件序列删除；

（3）将删除结果写⼊到DeleteOrphanFiles.Result中。

孤儿文件删除的相关类图如图5.19所示。
buildActionResult会将删除的文件封装起来交给下游的指标系统；
doExecute会调用Iceberg那边提供的删除孤儿文件接口进行实际的文件删除操作。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{RemoveOrphanFilesJob.png}
  \caption{孤儿文件删除相关类图}
  \label{fig:badge}
\end{figure}

\subsection{数据生命周期管理}

对于生命周期管理，我们需要用户设置时间字段timeField和过期时间expireTime，这就要求table schema
中有时间相关的字段，然后在实际运行时会根据每条数据的timeField的值和设置的expireTime进行比较，然后决定是否进行数据的删除。
生命周期管理的过程分为四步：

（1）扫描获取所有的dataFile；

（2）读取upperBounds中对应字段的值（dataFile中记录字段最大值）；

（3）计算过期时间最大值：ExpiredTime = dataStartTime - (durationInDays * millisecondInDays)，
如果ExpiredTime > upperBounds(field.fieldId())，大于则将此dataFile添加到deleteFile文件序列中；

（4）将deleteFile文件commit。

孤儿文件删除的相关类图如图5.20所示。timeField是所选的时间字段，timeFieldFormat是时间的格式；
durationInDays是数据持续的时间，需要用户设置，用来和dataStartTime计算得到ExpiredTime；
getDataStartTime是获取dataStartTime的，是任务启动时的时间；
buildActionResult会将删除的数据条数和失效时间封装起来交给下游的指标系统；
doExecute会根据上面的过程进行实际的数据删除操作。

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{dataTTLJob.png}
  \caption{生命周期管理相关类图}
  \label{fig:badge}
\end{figure}

\section{本章小结}

本章主要介绍了系统的详细设计，利用类图和流程图，对数据源管理、元数据管理、数据入湖、数据探索、自动优化服务
实现过程中的重难点进行了详细的阐述。
