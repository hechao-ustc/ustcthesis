% !TeX root = ../main.tex

\ustcsetup{
  keywords  = {数据湖, Iceberg, 实时湖仓, 自动优化},
  keywords* = {Data Lake, Iceberg, Real-time Lakehouse, Auto Optimize},
}

\begin{abstract}

近年来随着大数据、机器学习和5G等技术的飞速发展，数据规模不断增大，数据来源和类型也变得更加多元化。
同时随着企业对数据驱动业务需求的不断深入，也随着海量数据分析技术的成熟，数据仓库已成为企业内部数据洞察的标准服务。然而，
传统基于Hadoop技术的数据仓库从数据导入到数据分析每个环节都有较大的延迟，使得数据分析的时效性大大降低。
同时，对于数据分析场景的拓展，传统单一的数仓架构也无法满足多变的数据分析需求。
为此，业界也在探索新一代更为通用的实时数仓和数据湖架构。

经过对企业大数据发展现状的深入调研，充分了解企业所面临的需求和痛点，同时结合前沿开源技术发展现状，设计
并实现了基于Iceberg的数据湖分析系统（Data Lake Analytics，简称DLA)。
DLA提供了数据源管理、元数据管理、数据入湖、数据探索四大功能。
其中数据源管理是DLA的一个重要功能，它可以对不同类型的数据源进行有效的管理和维护，
而数据源是入湖任务的前提要求，通过注册相应数据源，用户可在创建入湖任务时选择对应的源表；
元数据管理主要对Iceberg表元数据进行管理和维护，而元数据是描述Iceberg表的，Iceberg表即入湖任务中的目标表；
数据入湖是DLA的核心流程功能，目的是用户通过该功能将源数据表流程化入湖
和查看已申请入湖任务执行情况；数据探索是基于Spark和Presto实现了数据的查询和分析使用功能。
在元数据管理中，DLA还提供了Iceberg表的自动优化服务，包括合并小文件、清理过期快照数据、
删除孤儿文件、生命周期管理，这项服务使得用户不再需要自己写程序来进行运维，降低了用户的
运维成本，可以一键启动该服务，并会根据表的若干指标及历史执行情况判断所需资源，并配有
告警机制，及时通知专业运维处理。

有了DLA，数据导入的时效性可以由传统数仓的T+1提升到现在的T+0，提供秒级至分钟级的准实时数据接入；
数据查询在Presto的帮助下，实现了秒级分析能力，能够在海量数据上进行实时的交互式查询，提升了整体业务转化的效率；
而且DLA还提供了存储层面的批流融合，使得构建数据分析架构时不再需要为流式计算和批计算配备不同的存储服务，
可以使用统一的分布式存储系统来存储数据。
目前，DLA已经上线运行，为企业用户提供了稳定可靠的一站式数据入湖服务，
并将多个业务由原来的天级别的数据导入时延变成现在的分钟级别，数据查询时延也由原来的分钟级别降到了现在的秒级查询，
将整体端到端的数据展示时间缩短到了分钟级。
而且，DLA还对接了企业内部的大数据计算平台、数据查询平台和报表平台等多个下游业务系统，满足了企业数据开发需求
，创造了巨大的价值。

\end{abstract}

\begin{abstract*}

In recent years, with the rapid development of technologies such as big data,
machine learning, and 5G, the scale of data has continued to increase, and the
sources and types of data have become more diversified. At the same time, as
enterprises continue to deepen their data-driven business needs, and with the
maturity of massive data analysis technology, data warehouses have become a
standard service for enterprise internal data insights. However, the traditional
data warehouse based on Hadoop technology has a large delay in every link from data
import to data analysis, which greatly reduces the timeliness of data analysis.
At the same time, for the expansion of data analysis scenarios, the traditional
single data warehouse architecture cannot meet the ever-changing data analysis
needs. To this end, the industry is also exploring a new generation of more general
real-time data warehouse and data lake architecture.

After in-depth research on the development status of enterprise big data, we fully
understand the needs and pain points faced by enterprises, and combined with the
development status of cutting-edge open source technology, we designed and implemented
a data lake analysis system (Data Lake Analytics, referred to as DLA) based on Iceberg.
DLA provides four functions: data source management, metadata management, data into the
lake, and data exploration. Among them, data source management is an important function
of DLA. It can effectively manage and maintain different types of data sources, and the
data source is the prerequisite for the task of entering the lake. By registering the
corresponding data source, the user can create the task of entering the lake. Select
the corresponding source table; metadata management mainly manages and maintains the
metadata of the Iceberg table, and the metadata describes the Iceberg table, and the
Iceberg table is the target table in the task of entering the lake; data entering the
lake is the core process function of DLA , the purpose is for users to process the
source data tables into the lake and view the execution status of the tasks that have
been applied for entering the lake through this function; data exploration is based on
Spark and Presto to realize the function of data query and analysis. In metadata management,
DLA also provides automatic optimization services for Iceberg tables, including merging
small files, clearing expired snapshot data, deleting orphan files, and lifecycle management.
This service eliminates the need for users to write their own programs for operation and
maintenance , which reduces the user's operation and maintenance costs, the service can be
started with one click, and the required resources will be judged according to several
indicators in the table and historical execution conditions, and an alarm mechanism is
equipped to notify professional operation and maintenance processing in time.

With DLA, the timeliness of data import can be improved from the T+1 of the traditional
data warehouse to the current T+0, providing quasi-real-time data access at the second to
minute level; with the help of Presto, the data query is realized in seconds Level analysis
capability, which can perform real-time interactive query on massive data, improving the
efficiency of overall business transformation; and DLA also provides batch flow integration
at the storage level, so that it is no longer necessary to build a data analysis architecture
for stream computing and Batch computing is equipped with different storage services, and a
unified distributed storage system can be used to store data. At present, DLA has been put
into operation, providing enterprise users with a stable and reliable one-stop data-into-the-lake
service, and changing the data import delay of multiple businesses from the original day-level
to the current minute-level, and the data query delay is also reduced. From the original minute
level to the current second level query, the overall end-to-end data display time is shortened
to the minute level. Moreover, DLA is also connected to multiple downstream business systems
such as the big data computing platform, data query platform, and reporting platform within the
enterprise, which meets the data development needs of the enterprise and creates huge value.

\end{abstract*}
